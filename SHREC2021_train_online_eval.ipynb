{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efgvaa9CeMJ1"
      },
      "outputs": [],
      "source": [
        "#mount google drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "main_folder = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import itertools\n",
        "import logging\n",
        "import glob\n",
        "\n",
        "from functools import partial\n",
        "from datetime import datetime\n",
        "from random import randint,shuffle\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jPGMH6J8eN_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Utils"
      ],
      "metadata": {
        "id": "U88HsOk3g9oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def denoise(coords, input_dim):\n",
        "    stroke = savgol_filter(coords[:, 0], 7, 3, mode='nearest')\n",
        "    for i in range(1, input_dim):\n",
        "        x_new = savgol_filter(coords[:, i], 7, 3, mode='nearest')\n",
        "        stroke = np.hstack([stroke.reshape(len(coords), -1), x_new.reshape(-1, 1)])\n",
        "    return stroke\n",
        "\n",
        "def denoise_all(stroke_all, input_dim):\n",
        "    \"\"\"\n",
        "    smoothing filter to mitigate some artifacts of the data collection\n",
        "    \"\"\"\n",
        "    stroke_new_all = []\n",
        "    for coords in stroke_all:\n",
        "        stroke = denoise(coords, input_dim)\n",
        "        stroke_new_all.append(stroke)\n",
        "    return stroke_new_all\n",
        "\n",
        "def interpolate_all(stroke_all, max_x_length, input_dim):\n",
        "    \"\"\"\n",
        "    interpolates strokes using cubic spline\n",
        "    \"\"\"\n",
        "    coords_all = []\n",
        "    for stroke in stroke_all:\n",
        "        coords = interpolate(stroke, max_x_length, input_dim)\n",
        "        coords_all.append(coords)\n",
        "    return coords_all\n",
        "\n",
        "def interpolate(stroke, max_x_length, input_dim):\n",
        "    coords = np.zeros([input_dim, max_x_length], dtype=np.float32)\n",
        "    if len(stroke) > 3:\n",
        "        for j in range(input_dim):\n",
        "            f_x = interp1d(np.arange(len(stroke)), stroke[:, j], kind='linear')\n",
        "            xx = np.linspace(0, len(stroke) - 1, max_x_length)\n",
        "            # xx = np.random.uniform(0,len(stroke)-1, max_x_length)\n",
        "            x_new = f_x(xx)\n",
        "            coords[j, :] = x_new\n",
        "    coords = np.transpose(coords)\n",
        "    return coords\n",
        "\n",
        "def multiplier(data,label,multi):\n",
        "    data_ = data\n",
        "    label_ = label\n",
        "    for i in range(multi):\n",
        "        data_ = np.concatenate((data_, data))\n",
        "        label_ = np.concatenate((label_, label))\n",
        "\n",
        "    data = data_\n",
        "    label = label_\n",
        "\n",
        "    return data,label\n",
        "\n",
        "\n",
        "def shuffle(data,label):\n",
        "    data = np.asarray(data,dtype = object)\n",
        "    shuffled_indexes = np.random.permutation(np.shape(data)[0])\n",
        "    data = data[shuffled_indexes]\n",
        "    label = label[shuffled_indexes]\n",
        "    return data, label\n",
        "\n",
        "\n",
        "def get_new_gesture_data(path,max_length,input_dim, data_synthesis = False):\n",
        "    new_gesture_list = []\n",
        "\n",
        "    for i, fname in enumerate(sorted(glob.glob(path))):\n",
        "        new_gesture = genfromtxt(fname, delimiter=',')\n",
        "        #     new_gesture = denoise(new_gesture,input_dim)\n",
        "        new_gesture = interpolate(new_gesture, max_length, input_dim)\n",
        "        new_gesture_list.append(new_gesture)\n",
        "\n",
        "    if data_synthesis:\n",
        "        synthetic_data = np.load('synthetic_data/synthetic_data.npy', allow_pickle=True)\n",
        "        new_gesture_list = np.concatenate((new_gesture_list, synthetic_data))\n",
        "    new_label_list = np.ones(len(new_gesture_list)) * 11\n",
        "    new_gesture_list , new_label_list = shuffle(new_gesture_list,new_label_list)\n",
        "    return new_gesture_list , new_label_list\n",
        "\n",
        "def train_test_split(data,label,new_gesture_list,new_label_list,split_num):\n",
        "\n",
        "    for _ in range(10):\n",
        "        data, label =  shuffle(data, label)\n",
        "\n",
        "    train_data, test_data = np.split(\n",
        "        data, [np.shape(data)[0] * 9 // 10]\n",
        "    )\n",
        "    train_label, test_label = np.split(\n",
        "        label, [np.shape(label)[0] * 9 // 10]\n",
        "    )\n",
        "    multi = 10\n",
        "\n",
        "    train_data = np.concatenate((train_data,new_gesture_list[:split_num]))\n",
        "    train_label = np.concatenate((train_label,new_label_list[:split_num]))\n",
        "    train_data, train_label = multiplier(train_data, train_label, multi)\n",
        "\n",
        "    test_data = np.concatenate((test_data,new_gesture_list[split_num:]))\n",
        "    test_label = np.concatenate((test_label,new_label_list[split_num:]))\n",
        "    test_data, test_label = multiplier(test_data, test_label, multi)\n",
        "\n",
        "    train_data, train_label = shuffle(train_data, train_label)\n",
        "\n",
        "    test_data, test_label = shuffle(test_data, test_label)\n",
        "\n",
        "    train_data = np.asarray(train_data, dtype=np.float32)\n",
        "    test_data = np.asarray(test_data, dtype=np.float32)\n",
        "\n",
        "    train_label = np.asarray(train_label, dtype=np.int32)\n",
        "    test_label = np.asarray(test_label, dtype=np.int32)\n",
        "\n",
        "    return train_data, train_label, test_data, test_label\n",
        "\n",
        "def confusion_matrix(save_path,new_gesture):\n",
        "    cm = np.load(save_path + '/confusion_matrix.npy')\n",
        "\n",
        "    classes = ['still', 'ticktock', 'shrink', 'push', 'peaceout', 'madriddles',\n",
        "               'grow', 'flamingo', 'execution', 'cheshiredance', 'caterpillar', new_gesture]\n",
        "\n",
        "    target_names = list(classes)\n",
        "    title = 'Confusion matrix'\n",
        "    cmap = None\n",
        "    normalize = True\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    # np.save(SAVE_PATH+ 'confusion_matrix.npy',cm)\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    # plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    # plt.title(title, fontsize=30)\n",
        "    # plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=90, fontsize=20)\n",
        "        plt.yticks(tick_marks, target_names, fontsize=20)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 10 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.imshow(cm, cmap=cmap)\n",
        "    # plt.grid(None)\n",
        "    plt.tight_layout()\n",
        "    # plt.ylabel('True label', fontsize=50)\n",
        "    # plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass), fontsize=30)\n",
        "    # plt.show()\n",
        "    plt.savefig('confusion_matrix.pdf')\n",
        "    # plt.close()\n",
        "\n",
        "\n",
        "def model_selection(Model,drop_out,max_length,input_dim ,output_dim):\n",
        "    model = ''\n",
        "\n",
        "    if Model == 'rnn_att_model':\n",
        "        model = rnn_att_model(max_length,\n",
        "                          input_dim,\n",
        "                          output_dim,dropout=drop_out)\n",
        "\n",
        "    if Model == 'resNet_LSTM':\n",
        "        model = resNet_LSTM(max_length,\n",
        "                          input_dim,\n",
        "                          output_dim,dropout=drop_out)\n",
        "\n",
        "    elif Model == 'resNet':\n",
        "        model = resNet(max_length,\n",
        "                          input_dim,\n",
        "                          output_dim)\n",
        "    elif Model == 'cnnModel':\n",
        "        model = cnnModel(max_length,\n",
        "                          input_dim,\n",
        "                          output_dim)\n",
        "    elif Model == 'Inception':\n",
        "        model = Inception(max_length,\n",
        "                          input_dim,\n",
        "                          output_dim)\n",
        "    elif Model == 'smallCnnModel':\n",
        "        model = smallCnnModel(max_length,\n",
        "                          input_dim,\n",
        "                          output_dim)\n",
        "\n",
        "    elif Model == 'LSTM_RES':\n",
        "        model = LSTM_RES(max_length,\n",
        "                          input_dim,\n",
        "                          output_dim,dropout=drop_out)\n",
        "\n",
        "    return  model\n",
        "\n",
        "\n",
        "def init_logging(log_dir, Model):\n",
        "    logging_level = logging.INFO\n",
        "\n",
        "    log_file = 'log_{}.txt'.format(Model)\n",
        "\n",
        "    log_file = os.path.join(log_dir, log_file)\n",
        "    if os.path.isfile(log_file):\n",
        "        os.remove(log_file)\n",
        "\n",
        "    logging.basicConfig(\n",
        "        filename=log_file,\n",
        "        level=logging_level,\n",
        "        format='[[%(asctime)s]] %(message)s',\n",
        "        datefmt='%m/%d/%Y %I:%M:%S %p'\n",
        "    )\n",
        "    logging.getLogger().addHandler(logging.StreamHandler())\n",
        "\n",
        "    return logging\n"
      ],
      "metadata": {
        "id": "5hysI4Logp9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Handling"
      ],
      "metadata": {
        "id": "WFUxjK-6g5CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_shrec_2021(path):\n",
        "    gesture_data = []\n",
        "    sequence_idx_list = []\n",
        "\n",
        "    for i, fname in enumerate(sorted(glob.glob(path))):\n",
        "        # print(fname)\n",
        "\n",
        "        idx = fname.split('/')[-1].split('.')[0]\n",
        "        # print(idx)\n",
        "\n",
        "        data = genfromtxt(fname, delimiter=';')\n",
        "        data_ = np.delete(data, -1, axis=1)\n",
        "        gesture_data.append(data_)\n",
        "        sequence_idx_list.append(int(idx))\n",
        "\n",
        "    gesture_data_sorted = [x for _, x in sorted(zip(sequence_idx_list, gesture_data))]\n",
        "\n",
        "    return gesture_data_sorted\n",
        "\n",
        "def read_annotation(fname_,gesture_data,max_length = 80):\n",
        "    gesture_name = ['ONE','TWO','THREE','FOUR','OK','MENU', 'LEFT',\n",
        "                    'RIGHT','CIRCLE','V','CROSS','GRAB','PINCH','TAP','DENY', 'KNOB','EXPAND']\n",
        "    with open(fname_) as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    gesture_name_list = []\n",
        "    start_idx_list = []\n",
        "    end_idx_list = []\n",
        "    annotation_idx_list = []\n",
        "    for i in range(len(lines)):\n",
        "\n",
        "        gesture_name_list.append(lines[i].split(';')[1:-1][::3])\n",
        "        start_idx_list.append(lines[i].split(';')[2:-1][::3])\n",
        "        end_idx_list.append(lines[i].split(';')[0:-1][::3][1:])\n",
        "        annotation_idx_list.append(int(lines[i].split(';')[0]))\n",
        "\n",
        "    gesture_name_list = [x for _, x in sorted(zip(annotation_idx_list, gesture_name_list))]\n",
        "    start_idx_list = [x for _, x in sorted(zip(annotation_idx_list, start_idx_list))]\n",
        "    end_idx_list = [x for _, x in sorted(zip(annotation_idx_list, end_idx_list))]\n",
        "\n",
        "\n",
        "    single_gesture = []\n",
        "    gesture_class = []\n",
        "\n",
        "    for i in range(len(gesture_data)):\n",
        "        for j in range(len(gesture_name_list[i])):\n",
        "            start_idx = int(start_idx_list[i][j])\n",
        "            end_idx = int(end_idx_list[i][j])\n",
        "            single_gesture.append(gesture_data[i][start_idx:start_idx+max_length])\n",
        "            gesture_class.append(gesture_name.index(gesture_name_list[i][j]))\n",
        "\n",
        "    return single_gesture, gesture_class\n",
        "\n",
        "def pad_data(input_dim, data,max_length):\n",
        "    data_padded = np.zeros([len(data),max_length,input_dim])\n",
        "    for i in range(len(data)):\n",
        "        if len(data[i]) <= max_length:\n",
        "            data_padded[i,:len(data[i])] = data[i]\n",
        "        if len(data[i]) > max_length:\n",
        "            data_padded[i] = data[i][:max_length]\n",
        "    return data_padded\n",
        "\n",
        "def select_joint(selected_joint,gesture_data,pos = True, quat= True):\n",
        "    split_joint_name = ['palm', 'thumbA', 'thumbB', 'thumbEnd', 'indexA', 'indexB', 'indexC', 'indexEnd', 'middleA', 'middleB', 'middleC', 'middleEnd', 'ringA', 'ringB', 'ringC', 'ringEnd', 'pinkyA', 'pinkyB', 'pinkyC', 'pinkyEnd']\n",
        "\n",
        "    selected_joint_idx = []\n",
        "    for x in selected_joint:\n",
        "        selected_joint_idx.append(split_joint_name.index(x))\n",
        "\n",
        "    idx_list_pos_x = np.multiply(selected_joint_idx,7)\n",
        "    idx_list_pos_y = np.multiply(selected_joint_idx,7)+1\n",
        "    idx_list_pos_z = np.multiply(selected_joint_idx,7)+2\n",
        "    idx_list_quat_x = np.multiply(selected_joint_idx,7)+3\n",
        "    idx_list_quat_y = np.multiply(selected_joint_idx,7)+4\n",
        "    idx_list_quat_z = np.multiply(selected_joint_idx,7)+5\n",
        "    idx_list_quat_w = np.multiply(selected_joint_idx,7)+6\n",
        "\n",
        "    idx_list_pos = np.concatenate([idx_list_pos_x,idx_list_pos_y,idx_list_pos_z])\n",
        "    idx_list_quat = np.concatenate([idx_list_quat_x,idx_list_quat_y,idx_list_quat_z,idx_list_quat_w])\n",
        "\n",
        "\n",
        "    if pos:\n",
        "        data = gesture_data[:,sorted(idx_list_pos)]\n",
        "    if quat:\n",
        "        data = gesture_data[:,sorted(idx_list_quat)]\n",
        "    if pos and quat:\n",
        "        data = gesture_data[:,sorted(np.concatenate([idx_list_pos,idx_list_quat]))]\n",
        "\n",
        "    return data\n",
        "\n",
        "def preprocess(data, xlist = [0, 3, 6, 9, 12, 15],nor_to_wrist=False, relative=False):\n",
        "    # relative replacement or not\n",
        "    # use center as track or not, if not, use wrist as track\n",
        "    length = np.shape(data)[0]\n",
        "    input_dim = np.shape(data)[1]\n",
        "\n",
        "    if relative:\n",
        "        if nor_to_wrist:\n",
        "            data = nortowrist(data, xlist)\n",
        "            data = relative_track_batch(length, input_dim, data)\n",
        "        else:\n",
        "            data = relative_track_batch(length, input_dim, data)\n",
        "    elif not relative:\n",
        "        if nor_to_wrist:\n",
        "            data = nortowrist(data, xlist)\n",
        "        else:\n",
        "            data = data\n",
        "\n",
        "    return data\n",
        "\n",
        "def relative_track_batch(length, input_dim, data):\n",
        "    ### do this every iteration to make first step replacement=0\n",
        "    temp = data[:, 0:input_dim]\n",
        "    lastplace = np.zeros([length, 1])\n",
        "    # x\n",
        "    lastplace[1: length, 0] = temp[0: length - 1, 0]\n",
        "    lastplace[0, 0] = temp[0, 0]\n",
        "    temp[0: length, 0] -= lastplace[:, 0]\n",
        "    # y\n",
        "    lastplace[1: length, 0] = temp[0: length - 1, 1]\n",
        "    lastplace[0, 0] = temp[0, 1]\n",
        "    temp[0: length, 1] -= lastplace[:, 0]\n",
        "    # z\n",
        "    lastplace[1: length, 0] = temp[0: length - 1, 2]\n",
        "    lastplace[0, 0] = temp[0, 2]\n",
        "    temp[0: length, 2] -= lastplace[:, 0]\n",
        "    temp = np.reshape(temp, [length, input_dim])\n",
        "    return temp\n",
        "\n",
        "\n",
        "def nortowrist(data, xlist):\n",
        "    ###### normalize to wrist\n",
        "    ylist = np.add(xlist, 1)\n",
        "    zlist = np.add(xlist, 2)\n",
        "\n",
        "    xc = data[:, 0]\n",
        "    yc = data[:, 1]\n",
        "    zc = data[:, 2]\n",
        "    data[:, xlist] -= np.tile(\n",
        "        np.reshape(xc, [-1, 1]), (1, len(xlist)))\n",
        "    data[:, ylist] -= np.tile(\n",
        "        np.reshape(yc, [-1, 1]), (1, len(ylist)))\n",
        "    data[:, zlist] -= np.tile(\n",
        "        np.reshape(zc, [-1, 1]), (1, len(zlist)))\n",
        "\n",
        "    return data\n",
        "\n"
      ],
      "metadata": {
        "id": "Ldm9AcbieY5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfdata(train_data, train_label, test_data, test_label, batch_size):\n",
        "\n",
        "\n",
        "    trainData =  tf.data.Dataset.from_tensor_slices(\n",
        "        (train_data, train_label)\n",
        "    )\n",
        "    trainData = trainData.shuffle(buffer_size=np.shape(train_data)[0])\n",
        "    trainData = trainData.batch(batch_size).prefetch(buffer_size=1)\n",
        "    trainsteps = np.shape(train_data)[0] // batch_size\n",
        "\n",
        "    testData =  tf.data.Dataset.from_tensor_slices(\n",
        "        (test_data, test_label)\n",
        "    )\n",
        "    testData = testData.shuffle(buffer_size=np.shape(test_data)[0])\n",
        "    testData = testData.batch(batch_size).prefetch(buffer_size=1)\n",
        "    teststeps = np.shape(test_data)[0] // batch_size\n",
        "\n",
        "    return trainData,trainsteps, testData, teststeps\n",
        "\n",
        "\n",
        "def getDataTest(test_data, test_label, batch_size):\n",
        "\n",
        "    testData =  tf.data.Dataset.from_tensor_slices(\n",
        "        (test_data, test_label)\n",
        "    )\n",
        "\n",
        "    testData = testData.batch(batch_size).prefetch(buffer_size=1)\n",
        "    teststeps = np.shape(test_data)[0] // batch_size\n",
        "\n",
        "    return testData, teststeps, test_data, test_label\n",
        "\n"
      ],
      "metadata": {
        "id": "rN7AVFp4iMal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Function Implementation"
      ],
      "metadata": {
        "id": "jeNC5i6oiDts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def train_classifier(model, Model, SAVE_PATH, EPOCHS,\n",
        "                learning_rate, train_data, train_label, test_data, test_label, batch_size):\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics=[\"sparse_categorical_accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "    SAVE_PATH_model = SAVE_PATH + 'Model/'\n",
        "\n",
        "    trainData, trainsteps, \\\n",
        "    validData, validsteps,\\\n",
        "        = tfdata(train_data, train_label, test_data, test_label, batch_size)\n",
        "\n",
        "\n",
        "    # Stop if the validation accuracy doesn't imporove for x epochs\n",
        "    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, verbose=2)\n",
        "\n",
        "    # Reduce LR on Plateau\n",
        "\n",
        "    reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, verbose=2)\n",
        "\n",
        "    history_lstm = model.fit(trainData.repeat(),\n",
        "                             steps_per_epoch=trainsteps,\n",
        "                             validation_data=validData.repeat(),\n",
        "                             validation_steps=validsteps,\n",
        "                             epochs=EPOCHS,\n",
        "                             # callbacks=[reduceLR,earlyStopping]\n",
        "                             #callbacks=[reduceLR]\n",
        "                             )\n",
        "\n",
        "    ## Save model\n",
        "    model.save(SAVE_PATH_model)\n",
        "\n",
        "    ## Save model\n",
        "    with open(SAVE_PATH_model + \"train_results.pickle\", \"wb\") as handle:\n",
        "        pickle.dump(history_lstm.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"Saved training history to res\")\n"
      ],
      "metadata": {
        "id": "gtoo3t9XiHfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Implementations"
      ],
      "metadata": {
        "id": "GHR6fAKQgydv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Models implementations\n",
        "'''\n",
        "def cnnModel(input_shape1,\n",
        "                input_shape2,\n",
        "                  output_shape):\n",
        "\n",
        "    input_shape = (input_shape1, input_shape2)\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    # Normalization layer\n",
        "    model.add(tf.keras.layers.Reshape(input_shape=input_shape, target_shape=(input_shape1, input_shape2, 1)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    filters = [16, 32, 64, 128]\n",
        "\n",
        "    for num_filters in filters:\n",
        "        # Conv a\n",
        "        model.add(tf.keras.layers.Conv2D(\n",
        "            num_filters,\n",
        "            kernel_size=(3, 3),\n",
        "            padding='same'\n",
        "            )\n",
        "        )\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.Activation('relu'))\n",
        "\n",
        "        # Conv b\n",
        "        model.add(tf.keras.layers.Conv2D(\n",
        "            num_filters,\n",
        "            kernel_size=(3, 3),\n",
        "            padding='same'\n",
        "            )\n",
        "        )\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.Activation('relu'))\n",
        "\n",
        "        # Pooling\n",
        "        model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "        model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "    # Classification layers\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(512, name='features512'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.4))\n",
        "    model.add(tf.keras.layers.Dense(256, name='features256'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.4))\n",
        "    model.add(tf.keras.layers.Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def smallCnnModel(input_shape1,\n",
        "                input_shape2,\n",
        "                  output_shape):\n",
        "\n",
        "\n",
        "    input_shape = (input_shape1, input_shape2)\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Reshape(input_shape=input_shape, target_shape=(input_shape1, input_shape2, 1)))\n",
        "\n",
        "    model.add(tf.keras.layers.Convolution2D(32, (1, 10), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.Convolution2D(64, (1, 5), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPool2D((1, 4)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "    model.add(tf.keras.layers.Convolution2D(64, (1, 10), padding='valid', activation='relu'))\n",
        "    model.add(tf.keras.layers.Convolution2D(128, (10, 1), padding='same', activation='relu'))\n",
        "\n",
        "    model.add(tf.keras.layers.GlobalMaxPool2D())\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dropout(0.4))\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu', name='FEATURES'))\n",
        "    model.add(tf.keras.layers.Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def modular_cnn_model(input_shape=(300, 18)):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Reshape(\n",
        "        input_shape=input_shape, target_shape=(input_shape[0], input_shape[1], 1)))\n",
        "\n",
        "    model.add(tf.keras.layers.Convolution2D(\n",
        "        32, (1, 5), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.Convolution2D(\n",
        "        64, (1, 5), padding='same', activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((1, 4)))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "    model.add(tf.keras.layers.Convolution2D(\n",
        "        64, (1, 5), padding='valid', activation='relu'))\n",
        "    model.add(tf.keras.layers.Convolution2D(\n",
        "        128, (5, 1), padding='same', activation='relu'))\n",
        "\n",
        "    model.add(tf.keras.layers.GlobalMaxPooling2D())\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dropout(0.4))\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu', name='FEATURES'))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "class inceptionModule(tf.keras.layers.Layer):\n",
        "    def __init__(self, n):\n",
        "        super(inceptionModule,self).__init__()\n",
        "        self.n = n\n",
        "    def __call__(self, x_input):\n",
        "\n",
        "        # Conv 1x1\n",
        "        conv_1x1 = tf.keras.layers.Conv2D(self.n, (1, 1), padding='same', activation='relu')(x_input)\n",
        "\n",
        "        # Conv 3x3\n",
        "        conv_3x3 = tf.keras.layers.Conv2D(self.n, (1, 1), padding='same', activation='relu')(x_input)\n",
        "        conv_3x3 = tf.keras.layers.Conv2D(self.n, (3, 3), padding='same', activation='relu')(conv_3x3)\n",
        "\n",
        "        # Conv 5x5\n",
        "        conv_5x5 = tf.keras.layers.Conv2D(self.n, (1, 1), padding='same', activation='relu')(x_input)\n",
        "        conv_5x5 = tf.keras.layers.Conv2D(self.n, (3, 3), padding='same', activation='relu')(conv_5x5)\n",
        "\n",
        "        # pool + proj\n",
        "        pool = tf.keras.layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x_input)\n",
        "        pool = tf.keras.layers.Conv2D(self.n, (1, 1), padding='same', activation='relu')(pool)\n",
        "\n",
        "        output = tf.keras.layers.concatenate([conv_1x1, conv_3x3, conv_5x5, pool], axis=3)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "def Inception(input_shape1,\n",
        "                input_shape2,\n",
        "                  output_shape):\n",
        "\n",
        "\n",
        "    input_shape = (input_shape1, input_shape2)\n",
        "    input_layer = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    reshape_layer = tf.keras.layers.Reshape(target_shape=(input_shape1, input_shape2, 1))(input_layer)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(32, (6, 4), padding='same', strides=(2, 2), activation='relu')(reshape_layer)\n",
        "    x = tf.keras.layers.MaxPooling2D((3, 2), padding='same', strides=(1, 1))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "    x = inceptionModule(32)(x)\n",
        "    x = inceptionModule(64)(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((3, 2))(x)\n",
        "\n",
        "    x = inceptionModule(64)(x)\n",
        "    x = inceptionModule(128)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(output_shape, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_layer], outputs=[x])\n",
        "    return model\n",
        "\n",
        "\n",
        "class residualModule(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_filters):\n",
        "        super(residualModule,self).__init__()\n",
        "        self.n_filters = n_filters\n",
        "        self.merge_input = tf.keras.layers.Conv2D(\n",
        "                self.n_filters, (1, 1), padding='same',\n",
        "                activation='relu',\n",
        "                kernel_initializer='he_normal')\n",
        "        self.conv1 = tf.keras.layers.Conv2D(\n",
        "            self.n_filters, (3, 3), padding='same',\n",
        "            activation='relu',\n",
        "            kernel_initializer='he_normal')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(\n",
        "            self.n_filters, (3, 3), padding='same',\n",
        "            activation='linear',\n",
        "            kernel_initializer='he_normal')\n",
        "        self.BatchNormalization = tf.keras.layers.BatchNormalization()\n",
        "        self.activation = tf.keras.layers.Activation('relu')\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def __call__(self, layer_in):\n",
        "        merge_input = layer_in\n",
        "        # check if the number of filters needs to be increased\n",
        "        if layer_in.shape[-1] != self.n_filters:\n",
        "            merge_input = self.merge_input(layer_in)\n",
        "\n",
        "        conv1 = self.conv1(layer_in)\n",
        "        conv2 = self.conv2(conv1)\n",
        "\n",
        "        # add filters\n",
        "        layer_out = self.add([conv2, merge_input])\n",
        "        layer_out = self.BatchNormalization(layer_out)\n",
        "        layer_out = self.activation(layer_out)\n",
        "\n",
        "        return layer_out\n",
        "\n",
        "\n",
        "\n",
        "def resNet(input_shape1,\n",
        "                input_shape2,\n",
        "                  output_shape):\n",
        "\n",
        "    input_shape = (input_shape1, input_shape2)\n",
        "\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    reshape_layer = tf.keras.layers.Reshape(input_shape=input_shape, target_shape=(input_shape1, input_shape2, 1))(input_layer)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(64, (6, 4), padding='same', strides=(2, 2), activation='relu')(reshape_layer)\n",
        "    x = tf.keras.layers.MaxPool2D((3, 2), padding='same', strides=(1, 1))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = residualModule(64)(x)\n",
        "    x = residualModule(128)(x)\n",
        "    x = tf.keras.layers.MaxPool2D((3, 2))(x)\n",
        "\n",
        "    x = residualModule(128)(x)\n",
        "    x = residualModule(256)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(output_shape, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_layer], outputs=[x])\n",
        "    return model\n",
        "\n",
        "\n",
        "def rnn_att_model(input_shape1,\n",
        "                input_shape2,\n",
        "                  output_shape,\n",
        "                  cnn_features=10,\n",
        "                  rnn='LSTM',\n",
        "                  multi_rnn=True,\n",
        "                  attention=True,\n",
        "                  dropout=0.2):\n",
        "\n",
        "    # Fetch input\n",
        "    input_shape = (input_shape1, input_shape2)\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    # reshape = tf.keras.layers.Reshape(\n",
        "    #     input_shape=input_shape, target_shape=(input_shape1, input_shape2, 1))(inputs)\n",
        "    # # Normalization Layer\n",
        "    # layer_out = tf.keras.layers.BatchNormalization()(reshape)\n",
        "    #\n",
        "    # # Convolutional Layer\n",
        "    # layer_out = tf.keras.layers.Conv2D(cnn_features, kernel_size=(3, 3),\n",
        "    #                                    padding='same', activation='relu')(layer_out)\n",
        "    # layer_out = tf.keras.layers.BatchNormalization()(layer_out)\n",
        "    # layer_out = tf.keras.layers.Conv2D(1, kernel_size=(3, 3),\n",
        "    #                                    padding='same', activation='relu')(layer_out)\n",
        "    # layer_out = tf.keras.layers.BatchNormalization()(layer_out)\n",
        "    # layer_out = tf.keras.layers.Lambda(\n",
        "    #     lambda x: tf.keras.backend.squeeze(x, -1), name='squeeze_dim')(layer_out)\n",
        "\n",
        "    # LSTM Layer\n",
        "    if rnn not in ['LSTM', 'GRU']:\n",
        "        raise ValueError(\n",
        "            'rnn should be equal to LSTM or GRU. No model generated...')\n",
        "\n",
        "    if rnn == 'LSTM':\n",
        "        layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "            400, return_sequences=True, dropout=dropout))(inputs)\n",
        "        if multi_rnn:\n",
        "            layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "                400, return_sequences=True, dropout=dropout))(layer_out)\n",
        "\n",
        "    # GRU Layer\n",
        "    if rnn == 'GRU':\n",
        "        layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "            400, return_sequences=True, dropout=dropout))(inputs)\n",
        "        if multi_rnn:\n",
        "            layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "                400, return_sequences=True, dropout=dropout))(layer_out)\n",
        "\n",
        "    # Attention Layer\n",
        "    if attention:\n",
        "        query, value = tf.keras.layers.Lambda(\n",
        "            lambda x: tf.split(x, num_or_size_splits=2, axis=2))(layer_out)\n",
        "        layer_out = tf.keras.layers.Attention(name='Attention')([query, value])\n",
        "\n",
        "    # Classification Layer\n",
        "    outputs = tf.keras.layers.Flatten()(layer_out)\n",
        "    outputs = tf.keras.layers.Dense(512, activation='relu')(outputs)\n",
        "    outputs = tf.keras.layers.Dense(output_shape, activation='softmax')(outputs)\n",
        "\n",
        "    # Output Model\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "\n",
        "def resNet_LSTM(input_shape1,\n",
        "                input_shape2,\n",
        "                  output_shape,\n",
        "                  cnn_features=10,\n",
        "                  rnn='LSTM',\n",
        "                  multi_rnn=True,\n",
        "                  attention=True,\n",
        "                  dropout=0.2):\n",
        "    \"\"\"\n",
        "    Neural network with residual blocks\n",
        "    Accuracy = 0.96\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = (input_shape1, input_shape2)\n",
        "\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    reshape_layer = tf.keras.layers.Reshape(input_shape=input_shape, target_shape=(input_shape1, input_shape2, 1))(input_layer)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(64, (6, 4), padding='same', strides=(2, 2), activation='relu')(reshape_layer)\n",
        "    x = tf.keras.layers.MaxPool2D((3, 2), padding='same', strides=(1, 1))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = residualModule(64)(x)\n",
        "    #x = residualModule(128)(x)\n",
        "    x = tf.keras.layers.MaxPool2D((3, 2))(x)\n",
        "\n",
        "    x = residualModule(128)(x)\n",
        "    #x = residualModule(256)(x)\n",
        "\n",
        "    layer_out = tf.keras.layers.Conv2D(1, kernel_size=(3, 3),\n",
        "                                       padding='same', activation='relu')(x)\n",
        "    layer_out = tf.keras.layers.BatchNormalization()(layer_out)\n",
        "    layer_out = tf.keras.layers.Lambda(\n",
        "        lambda x: tf.keras.backend.squeeze(x, -1), name='squeeze_dim')(layer_out)\n",
        "\n",
        "    x = tf.keras.layers.Dense(218, activation='relu')(layer_out)\n",
        "\n",
        "    # LSTM Layer\n",
        "    if rnn not in ['LSTM', 'GRU']:\n",
        "        raise ValueError(\n",
        "            'rnn should be equal to LSTM or GRU. No model generated...')\n",
        "\n",
        "    if rnn == 'LSTM':\n",
        "        layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "            400, return_sequences=True, dropout=dropout))(x)\n",
        "        if multi_rnn:\n",
        "            layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "                400, return_sequences=True, dropout=dropout))(layer_out)\n",
        "\n",
        "    # GRU Layer\n",
        "    if rnn == 'GRU':\n",
        "        layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "            400, return_sequences=True, dropout=dropout))(x)\n",
        "        if multi_rnn:\n",
        "            layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "                400, return_sequences=True, dropout=dropout))(layer_out)\n",
        "\n",
        "    # Attention Layer\n",
        "    if attention:\n",
        "        query, value = tf.keras.layers.Lambda(\n",
        "            lambda x: tf.split(x, num_or_size_splits=2, axis=2))(layer_out)\n",
        "        layer_out = tf.keras.layers.Attention(name='Attention')([query, value])\n",
        "\n",
        "    # Classification Layer\n",
        "    outputs = tf.keras.layers.Flatten()(layer_out)\n",
        "    outputs = tf.keras.layers.Dense(512, activation='relu')(outputs)\n",
        "    outputs = tf.keras.layers.Dense(output_shape, activation='softmax')(outputs)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_layer], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def LSTM_RES(input_shape1,\n",
        "                input_shape2,\n",
        "                  output_shape,\n",
        "                  cnn_features=10,\n",
        "                  rnn='LSTM',\n",
        "                  multi_rnn=True,\n",
        "                  attention=True,\n",
        "                  dropout=0.2):\n",
        "    \"\"\"\n",
        "    Neural network with residual blocks\n",
        "    Accuracy = 0.96\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = (input_shape1, input_shape2)\n",
        "\n",
        "    input_layer = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "\n",
        "    # LSTM Layer\n",
        "    if rnn not in ['LSTM', 'GRU']:\n",
        "        raise ValueError(\n",
        "            'rnn should be equal to LSTM or GRU. No model generated...')\n",
        "\n",
        "    if rnn == 'LSTM':\n",
        "        layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "            400, return_sequences=True, dropout=dropout))(input_layer)\n",
        "        if multi_rnn:\n",
        "            layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "                400, return_sequences=True, dropout=dropout))(layer_out)\n",
        "\n",
        "    # GRU Layer\n",
        "    if rnn == 'GRU':\n",
        "        layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "            400, return_sequences=True, dropout=dropout))(input_layer)\n",
        "        if multi_rnn:\n",
        "            layer_out = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
        "                400, return_sequences=True, dropout=dropout))(layer_out)\n",
        "\n",
        "    # Attention Layer\n",
        "    if attention:\n",
        "        query, value = tf.keras.layers.Lambda(\n",
        "            lambda x: tf.split(x, num_or_size_splits=2, axis=2))(layer_out)\n",
        "        layer_out = tf.keras.layers.Attention(name='Attention')([query, value])\n",
        "\n",
        "\n",
        "    reshape_layer = tf.keras.layers.Reshape(input_shape=[input_shape1,400], target_shape=(input_shape1, 400, 1))(layer_out)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(64, (6, 4), padding='same', strides=(2, 2), activation='relu')(reshape_layer)\n",
        "    x = tf.keras.layers.MaxPool2D((3, 2), padding='same', strides=(1, 1))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = residualModule(64)(x)\n",
        "    x = residualModule(128)(x)\n",
        "    x = tf.keras.layers.MaxPool2D((3, 2))(x)\n",
        "\n",
        "    x = residualModule(128)(x)\n",
        "    x = residualModule(256)(x)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(output_shape, activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_layer], outputs=[x])\n",
        "    return model"
      ],
      "metadata": {
        "id": "88BvuOSFguCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '{}/test_set/sequences/*.txt'.format(main_folder)\n",
        "gesture_data_test = read_shrec_2021(path)\n",
        "path = '{}/training_set/sequences/*.txt'.format(main_folder)\n",
        "gesture_data_train = read_shrec_2021(path)"
      ],
      "metadata": {
        "id": "TRI9n9sDeT28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 250\n",
        "\n",
        "fname_ = '{}/training_set/annotations_revised_training.txt'.format(main_folder)\n",
        "single_gesture_train, gesture_class_train = read_annotation(fname_,gesture_data_train,max_length)\n",
        "fname_ = '{}/test_set/annotations_revised.txt'.format(main_folder)\n",
        "single_gesture_test, gesture_class_test = read_annotation(fname_,gesture_data_test,max_length)"
      ],
      "metadata": {
        "id": "YyV6bDTUfCWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "index = 48\n",
        "output_dir = '{}/processed_test'.format(main_folder)\n",
        "gesture_name = ['ONE','TWO','THREE','FOUR','OK','MENU', 'LEFT','RIGHT','CIRCLE','V','CROSS','GRAB','PINCH','TAP','DENY', 'KNOB','EXPAND']\n",
        "selected_classes = ['ONE','TWO','THREE','FOUR','OK','MENU', 'LEFT','RIGHT','CIRCLE','V','CROSS','GRAB','PINCH','TAP','DENY', 'KNOB','EXPAND']\n",
        "# select specific classes to form the training data, this is to simulate those non selected classes as non-gestures\n",
        "selected_classes_idx = [gesture_name.index(x) for x in selected_classes]\n",
        "for i in range(len(gesture_class_train)):\n",
        "    if gesture_class_train[i] in selected_classes_idx:\n",
        "        index+=1\n",
        "        output_filename = '{}/{}_{}.csv'.format(output_dir, index,gesture_class_train[i])\n",
        "        # print(output_filename)\n",
        "        np.savetxt(output_filename, single_gesture_train[i], fmt='%1.4f', delimiter=\",\")"
      ],
      "metadata": {
        "id": "7KSdb3eNfTJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_joint =['palm', 'thumbA', 'thumbB', 'thumbEnd', 'indexA', 'indexB', 'indexC', 'indexEnd', 'middleA', 'middleB', 'middleC', 'middleEnd', 'ringA', 'ringB', 'ringC', 'ringEnd', 'pinkyA', 'pinkyB', 'pinkyC', 'pinkyEnd']\n",
        "# Select specific joints from the training and testing data\n",
        "selected_gesture_train = [select_joint(selected_joint,x,pos = True, quat= True) for x in single_gesture_train]\n",
        "selected_gesture_test = [select_joint(selected_joint,x,pos = True, quat= True) for x in single_gesture_test]"
      ],
      "metadata": {
        "id": "duXYybSgfy-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the model\n",
        "Model= 'resNet'\n",
        "# set model and data parameters\n",
        "max_length = 250\n",
        "batch_size = 64\n",
        "input_dim = len(selected_joint)*7\n",
        "output_dim = 17\n",
        "EPOCHS = 100\n",
        "learning_rate = 0.0001\n",
        "drop_out = 0.3"
      ],
      "metadata": {
        "id": "0uW_9-VMgCL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pad_data(input_dim, selected_gesture_train,max_length)\n",
        "assert not np.any(np.isnan(train_data))\n",
        "test_data = pad_data(input_dim,selected_gesture_test,max_length)\n",
        "assert not np.any(np.isnan(test_data))\n",
        "train_label, test_label = gesture_class_train, gesture_class_test"
      ],
      "metadata": {
        "id": "M-Hyph90gRRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_data, train_label = multiplier(train_data, train_label, multi=1)\n",
        "\n",
        "train_data, train_label = shuffle(train_data, train_label)\n",
        "\n",
        "\n",
        "train_data = np.asarray(train_data, dtype=np.float32)\n",
        "test_data = np.asarray(test_data, dtype=np.float32)\n",
        "\n",
        "train_label = np.asarray(train_label, dtype=np.int32)\n",
        "test_label = np.asarray(test_label, dtype=np.int32)\n",
        "\n",
        "train_data, train_label = shuffle(train_data, train_label)\n",
        "test_data, test_label = shuffle(test_data, test_label)\n",
        "\n",
        "train_data = np.asarray(train_data, dtype=np.float32)\n",
        "test_data = np.asarray(test_data, dtype=np.float32)\n",
        "\n",
        "train_label = np.asarray(train_label, dtype=np.int32)\n",
        "test_label = np.asarray(test_label, dtype=np.int32)\n"
      ],
      "metadata": {
        "id": "NdlBARxp3-un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaPclQEPgTAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a model and set the model parameters\n",
        "model = model_selection(Model,drop_out,max_length,input_dim ,output_dim)\n",
        "\n",
        "save_path = main_folder\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# log info\n",
        "\n",
        "print('max_length {}'.format(max_length))\n",
        "print('batch_size {}'.format(batch_size))\n",
        "print('EPOCHS {}'.format(str(EPOCHS)))\n",
        "print('learning_rate {}'.format(str(learning_rate)))\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "VIZexYrygf1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "tnCV6PqZiQnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_classifier(\n",
        "    model,\n",
        "    Model,\n",
        "    save_path,\n",
        "    EPOCHS,\n",
        "    learning_rate,\n",
        "    train_data,\n",
        "    train_label,\n",
        "    test_data,\n",
        "    test_label,\n",
        "    batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "UQKYu6pagjPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpkKcl0I_UiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Online Evaluation"
      ],
      "metadata": {
        "id": "PvHUZTlPiaaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import editdistance\n",
        "\n",
        "\n",
        "\n",
        "def load_model(save_path):\n",
        "    SAVE_PATH_model = save_path + '/Model'\n",
        "    model = keras.models.load_model(SAVE_PATH_model, custom_objects={'tf': tf})\n",
        "    print('loaded model from {}'.format(save_path))\n",
        "    return model\n",
        "\n",
        "def online_rec(model,max_length, window_step, input_test):\n",
        "    window_width = max_length\n",
        "    outputs_list = []\n",
        "    y_pred_list = []\n",
        "    y_pred_value_list = []\n",
        "\n",
        "    for window in np.arange(0, int(len(input_test) - window_width) + 1, window_step):\n",
        "        window = int(window)\n",
        "\n",
        "        input_slice = np.expand_dims(input_test[window:window + window_width], axis=0)\n",
        "\n",
        "        Data = tf.data.Dataset.from_tensor_slices(\n",
        "            (input_slice)\n",
        "        )\n",
        "        Data = Data.batch(1).prefetch(buffer_size=1)\n",
        "\n",
        "        outputs = model.predict(Data, verbose=0)\n",
        "        outputs_softmax = tf.nn.softmax(outputs)\n",
        "        y_pred = np.argmax(outputs_softmax, axis=1)\n",
        "        y_pred_value = np.max(outputs_softmax)\n",
        "\n",
        "        outputs_list.append(outputs_softmax)\n",
        "        y_pred_list.append(int(y_pred))\n",
        "        y_pred_value_list.append(float(y_pred_value))\n",
        "\n",
        "    return y_pred_list, outputs_list, y_pred_value_list\n",
        "\n",
        "\n",
        "def eval_accuracy_nttd(y_pred_list, y_true, y_pred_value_list, threshold=0.6, recur_threshold=20,max = 0.21, min = 0.08):\n",
        "    pred_list = []\n",
        "    y_pred_value_list_ = (np.array(y_pred_value_list) - min) / (max - min)\n",
        "\n",
        "    recur = 0\n",
        "    frame_idx = []\n",
        "    y_pred_ = y_pred_list[0]\n",
        "    for i, value in enumerate(y_pred_value_list_):\n",
        "\n",
        "        if value > threshold:\n",
        "            recur += 1\n",
        "            if recur > recur_threshold:\n",
        "                y_pred = int(y_pred_list[i])\n",
        "                if y_pred != y_pred_:\n",
        "                    if y_pred != 0:\n",
        "                        y_pred_ = y_pred\n",
        "                        frame_idx.append(i)\n",
        "                        frame_idx.append(i + 1)\n",
        "                        pred_list.append(y_pred)\n",
        "                        recur = 0\n",
        "\n",
        "    y_pred = np.asarray(pred_list, dtype=np.int64)\n",
        "\n",
        "    y_true = np.asarray(y_true, dtype=np.int64)\n",
        "\n",
        "    accuracy = 1 - editdistance.eval(y_pred.tostring(), y_true.tostring()) / len(y_true.tostring())\n",
        "\n",
        "    print('Pred {}'.format(y_pred))\n",
        "    print('True {}'.format(y_true))\n",
        "    print('Accuracy {}'.format(accuracy))\n",
        "\n",
        "    return y_pred, y_true, accuracy, frame_idx\n",
        "\n",
        "def evaluate_NTtD(classes,frame_idx,frame_sequence,y_true,y_pred):\n",
        "    frame_pred = (np.array(frame_idx) + 300)[::2]\n",
        "    frame_true = frame_sequence\n",
        "    for j, b in enumerate(y_true):\n",
        "        for i, a in enumerate(y_pred):\n",
        "            start = frame_true[j * 2]\n",
        "            end = frame_true[j * 2 + 1]\n",
        "            #         print(frame_pred[i])\n",
        "\n",
        "            if a == b and start < frame_pred[i] < end:\n",
        "                NTtD = (frame_pred[i] - start + 1) / (end - start + 1)\n",
        "                print('{} is recognized correctly at {} seconds, with NTtD {}'.format(classes[b].upper(),\n",
        "                                                                                      frame_pred[i] / 72, NTtD))\n",
        "\n",
        "\n",
        "def online_plot(outputs_list, input_test,input_label, frame_sequence):\n",
        "\n",
        "    outputs_list_ = np.squeeze(outputs_list, axis=1)\n",
        "    color_list_11 = ['b-', 'g-', 'r-', 'c-', 'm-', 'y-', 'k-',\n",
        "                     'b--', 'g--', 'r--', 'c--', 'm--', 'y--', 'k--',\n",
        "                     'b-.', 'g-.', 'r-.', 'c-.', 'm-.', 'y-.', 'k-.',\n",
        "                     'b:', 'g:', 'r:', 'c:', 'm:', 'y:', 'k:', 'b.']\n",
        "\n",
        "    list_ = np.zeros((17, len(input_test)))\n",
        "    for i, index in enumerate(frame_sequence):\n",
        "        if i % 2 == 0:\n",
        "            list_[input_label[int(i / 2)]][frame_sequence[i]:frame_sequence[i + 1]] = 1\n",
        "\n",
        "    list__ = np.transpose(list_)\n",
        "\n",
        "    x_axis = np.arange(np.shape(outputs_list_)[0])\n",
        "    x_axis_tile = np.tile(x_axis, (np.shape(outputs_list_)[1], 1))\n",
        "    x_axis_tile_t = np.transpose(x_axis_tile)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)\n",
        "\n",
        "    for i in range(17):\n",
        "        if i == 0:\n",
        "            continue\n",
        "        ax1.plot(np.concatenate((np.arange(300) / 72, x_axis_tile_t[:, i] / 72 + 300 / 72)),\n",
        "                 np.concatenate((np.ones(300) *np.mean(outputs_list_[:, 0]), outputs_list_[:, i])), color_list_11[i])\n",
        "\n",
        "    for i in range(17):\n",
        "        if i == 0:\n",
        "            continue\n",
        "        ax2.plot(np.concatenate((np.arange(300) / 72, x_axis_tile_t[:-1, i] / 72 + 300 / 72))[:len(list__)], list__[:, i],\n",
        "                 color_list_11[i])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # fig.savefig('plot/online_{}.png'.format('_'.join([str(i) for i in input_label])))"
      ],
      "metadata": {
        "id": "BtzWCm0AkHF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, accuracy_score,\n",
        "    precision_score, recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "\n",
        "def eval_classifier(Model,SAVE_PATH,test_data, test_label, batch_size):\n",
        "\n",
        "    ## test load\n",
        "    Data,_,_, y_true  = getDataTest(test_data, test_label, batch_size)\n",
        "    SAVE_PATH_model = SAVE_PATH + 'Model'\n",
        "    model = keras.models.load_model(SAVE_PATH_model, custom_objects={'tf': tf})\n",
        "    print('Model loaded: {}'.format(Model))\n",
        "\n",
        "    out = model.predict(Data, verbose=1)\n",
        "    y_pred = np.argmax(out,axis=1)\n",
        "    y_true = y_true.astype(np.int32)\n",
        "\n",
        "    print('Real Data for validation')\n",
        "    print('Accuracy: {:.4f}'.format(accuracy_score(y_pred, y_true)))\n",
        "    print('Precision: {:.4f}'.format(precision_score(y_pred, y_true, average='macro')))\n",
        "    print('Recall: {:.4f}'.format(recall_score(y_pred, y_true, average='macro')))\n",
        "    print('F1 score: {:.4f}'.format(f1_score(y_pred, y_true, average='macro')))\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('\\n')\n",
        "    cm = confusion_matrix(y_pred, y_true)\n",
        "    np.save(SAVE_PATH + '/confusion_matrix.npy', cm)\n",
        "\n",
        "    return out,[accuracy_score(y_pred, y_true), precision_score(y_pred, y_true, average='macro'),\n",
        "            recall_score(y_pred, y_true, average='macro'), f1_score(y_pred, y_true, average='macro') ]\n",
        "\n",
        "\n",
        "def confusion_matrix_(cm,save_path,classes):\n",
        "    cm = np.load(save_path + '/confusion_matrix.npy')\n",
        "\n",
        "\n",
        "    target_names = list(classes)\n",
        "    title = 'Confusion matrix'\n",
        "    cmap = None\n",
        "    normalize = True\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    # np.save(SAVE_PATH+ 'confusion_matrix.npy',cm)\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    # plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    # plt.title(title, fontsize=30)\n",
        "    # plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=90, fontsize=20)\n",
        "        plt.yticks(tick_marks, target_names, fontsize=20)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 10 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.imshow(cm, cmap=cmap)\n",
        "    # plt.grid(None)\n",
        "    plt.tight_layout()\n",
        "    # plt.ylabel('True label', fontsize=50)\n",
        "    # plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass), fontsize=30)\n",
        "    # plt.show()\n",
        "#     plt.savefig('shrec21_confusion_matrix.pdf')\n",
        "    # plt.close()\n"
      ],
      "metadata": {
        "id": "9A0ifcJXifiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(save_path)"
      ],
      "metadata": {
        "id": "ZK0VQgEirX9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating\n",
        "_,_ = eval_classifier(Model,save_path, test_data, test_label,batch_size)"
      ],
      "metadata": {
        "id": "PiCQ_F-hpZ3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['ONE','TWO','THREE','FOUR','OK','MENU', 'LEFT','RIGHT','CIRCLE','V','CROSS','GRAB','PINCH','TAP','DENY', 'KNOB','EXPAND']"
      ],
      "metadata": {
        "id": "iDnDY6tdi8vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = np.load(save_path + '/confusion_matrix.npy')\n",
        "cm1 = np.concatenate(([[0 , 0 ,0  ,0 , 0 , 0  ,0 , 0 , 0 , 0 , 0 , 0,  0 , 0 , 0 , 0  ,0]],cm),0)\n",
        "cm2=np.concatenate(([[16 , 0 ,0  ,0 , 0 , 0  ,0 , 0 , 0 , 0 , 0 , 0,  0 , 0 , 0 , 0  ,0,0]],cm1.T),0).T"
      ],
      "metadata": {
        "id": "px7-5Dtvi-7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_(cm2,save_path,classes)"
      ],
      "metadata": {
        "id": "8kuYy_MVjFkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_annotation_idx(fname_,idx):\n",
        "    gesture_name = ['ONE','TWO','THREE','FOUR','OK','MENU', 'LEFT',\n",
        "                    'RIGHT','CIRCLE','V','CROSS','GRAB','PINCH','TAP','DENY', 'KNOB','EXPAND']\n",
        "    with open(fname_) as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    gesture_name_list = []\n",
        "    start_idx_list = []\n",
        "    end_idx_list = []\n",
        "    annotation_idx_list = []\n",
        "    for i in range(len(lines)):\n",
        "\n",
        "        gesture_name_list.append(lines[i].split(';')[1:-1][::3])\n",
        "        start_idx_list.append(lines[i].split(';')[2:-1][::3])\n",
        "        end_idx_list.append(lines[i].split(';')[0:-1][::3][1:])\n",
        "        annotation_idx_list.append(int(lines[i].split(';')[0]))\n",
        "\n",
        "    gesture_name_list = [x for _, x in sorted(zip(annotation_idx_list, gesture_name_list))]\n",
        "    start_idx_list = [x for _, x in sorted(zip(annotation_idx_list, start_idx_list))]\n",
        "    end_idx_list = [x for _, x in sorted(zip(annotation_idx_list, end_idx_list))]\n",
        "\n",
        "    label = gesture_name_list[idx]\n",
        "    start_idx_list_ = start_idx_list[idx]\n",
        "    end_idx_list_ = end_idx_list[idx]\n",
        "    frame_sequence = [[int(start_idx_list_[i]),int(end_idx_list_[i])] for i in range(len(start_idx_list_))]\n",
        "\n",
        "    return label, frame_sequence"
      ],
      "metadata": {
        "id": "U2YWMXXSjLa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = True\n",
        "quat = True\n",
        "\n",
        "fname_ = '{}/test_set/annotations_revised.txt'.format(main_folder)\n",
        "\n",
        "idx = 109\n",
        "filename = '{}/test_set/sequences/{}.txt'.format(main_folder,idx)\n",
        "data_ = genfromtxt(filename, delimiter=';')\n",
        "data = np.delete(data_, -1, axis=1)\n",
        "# 180;THREE;185;303;LEFT;514;534;PINCH;815;843;ONE;1047;1112;KNOB;1298;1320;\n",
        "data = select_joint(selected_joint,data,pos, quat)\n",
        "label, frame_sequence = read_annotation_idx(fname_,idx-109)\n",
        "frame_sequence = np.reshape(frame_sequence,(-1))\n",
        "\n",
        "y_true = [classes.index(x) for x in label]\n",
        "print(frame_sequence)\n",
        "print(y_true)\n",
        "print(label)"
      ],
      "metadata": {
        "id": "1wdz1Cy3jNjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "def online_rec(model,max_length, window_step, input_test):\n",
        "    window_width = max_length\n",
        "    outputs_list = []\n",
        "    y_pred_list = []\n",
        "    y_pred_value_list = []\n",
        "\n",
        "    for window in tqdm.tqdm(np.arange(0, int(len(input_test) - window_width) + 1, window_step)):\n",
        "        window = int(window)\n",
        "\n",
        "        input_slice = np.expand_dims(input_test[window:window + window_width], axis=0)\n",
        "\n",
        "        Data = tf.data.Dataset.from_tensor_slices(\n",
        "            (input_slice)\n",
        "        )\n",
        "        Data = Data.batch(1).prefetch(buffer_size=1)\n",
        "\n",
        "        outputs = model.predict(Data, verbose=0)\n",
        "        outputs_softmax = tf.nn.softmax(outputs)\n",
        "        y_pred = np.argmax(outputs_softmax, axis=1)\n",
        "        y_pred_value = np.max(outputs_softmax)\n",
        "\n",
        "        outputs_list.append(outputs_softmax)\n",
        "        y_pred_list.append(int(y_pred))\n",
        "        y_pred_value_list.append(float(y_pred_value))\n",
        "\n",
        "    return y_pred_list, outputs_list, y_pred_value_list"
      ],
      "metadata": {
        "id": "eS-MaE6Tj2p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_step = 1\n",
        "y_pred_list, outputs_list, y_pred_value_list = online_rec(model,max_length, window_step, data)"
      ],
      "metadata": {
        "id": "y_oSvU_3j44V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, y_true, accuracy, frame_idx= eval_accuracy_nttd(y_pred_list, y_true, y_pred_value_list, threshold=0.5, recur_threshold=10, max = 0.14, min = 0.06)"
      ],
      "metadata": {
        "id": "eQdz7HJDj7-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_NTtD(classes,frame_idx,frame_sequence,y_true,y_pred)"
      ],
      "metadata": {
        "id": "EIeyPmsBj_Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "online_plot(outputs_list, data,y_true, frame_sequence)"
      ],
      "metadata": {
        "id": "NBwcni6XrskH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdymJ3Car7n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xAQTeX4A1kB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}